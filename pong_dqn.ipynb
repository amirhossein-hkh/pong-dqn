{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pong_dqn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nudb3jvgsbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import cv2\n",
        "from collections import deque,namedtuple\n",
        "import numpy as np\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,Flatten,Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "conv = namedtuple('Conv', 'filter kernel stride')\n",
        "\n",
        "class Buffer:\n",
        "\tdef __init__(self,size):\n",
        "\t\tself.size = size\n",
        "\t\tself.buffer = deque()\n",
        "\tdef add(self,s,a,r,s2,t):\n",
        "\t\ts = np.stack((s[0],s[1],s[2],s[3]),axis=2)\n",
        "\t\ts2 = np.stack((s2[0],s2[1],s2[2],s2[3]),axis=2)\n",
        "\t\tif len(self.buffer) < self.size:\n",
        "\t\t\tself.buffer.appendleft((s,a,r,s2,t))\n",
        "\t\telse:\n",
        "\t\t\tself.buffer.pop()\n",
        "\t\t\tself.buffer.appendleft((s,a,r,s2,t))\n",
        "\tdef sample(self,batch_size):\n",
        "\t\treturn random.sample(self.buffer,batch_size)\n",
        "\n",
        "class DQN:\n",
        "\tdef __init__(self,buff,batch_size=32,min_buff=10000,gamma=0.99,learning_rate=2.5e-4):\n",
        "\t\tself.buffer = buff\n",
        "\t\tself.min_buffer = min_buff\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.gamma = gamma\n",
        "\t\t\n",
        "\t\tself.model = create_network(learning_rate)\n",
        "\t\tself.target_model = create_network(learning_rate)\n",
        "\t\tself.copy_network()\n",
        "\n",
        "\tdef train(self):\n",
        "\t\tif len(self.buffer.buffer) < self.min_buffer:\n",
        "\t\t\treturn\n",
        "\t\tstates,actions,rewards,next_states,terminal = map(np.array,zip(*self.buffer.sample(self.batch_size)))\n",
        "\t\tnext_state_action_values = np.max(self.target_model.predict(next_states),axis=1)\n",
        "\t\ttargets = self.model.predict(states)\n",
        "\t\ttargets[range(self.batch_size), actions] = rewards + self.gamma*next_state_action_values*np.invert(terminal)\n",
        "\t\tself.model.train_on_batch(states, targets)\n",
        "\n",
        "\tdef copy_network(self):\n",
        "\t\tfrm = self.model\n",
        "\t\tto = self.target_model\n",
        "\t\tfor l_tg,l_sr in zip(to.layers,frm.layers):\n",
        "\t\t\twk = l_sr.get_weights()\n",
        "\t\t\tl_tg.set_weights(wk)\n",
        "\n",
        "\tdef predict(self,x):\n",
        "\t\ts = np.stack((x[0],x[1],x[2],x[3]),axis=2)\n",
        "\t\treturn self.model.predict(np.array([s]))\n",
        "\t\t\n",
        "def create_network(learning_rate,conv_info=[conv(32,8,4),conv(64,4,2),conv(64,3,1)],dense_info=[512],input_size=(80,80,4)):\n",
        "\tmodel = Sequential()\n",
        "\tfor i,cl in enumerate(conv_info):\n",
        "\t\tif i==0:\n",
        "\t\t\tmodel.add(Conv2D(cl.filter,cl.kernel,padding=\"same\",strides=cl.stride,activation=\"relu\", input_shape=input_size))\n",
        "\t\telse:\n",
        "\t\t\tmodel.add(Conv2D(cl.filter,cl.kernel,padding=\"same\",strides=cl.stride,activation=\"relu\"))\n",
        "\tmodel.add(Flatten())\n",
        "\tfor dl in dense_info:\n",
        "\t\tmodel.add(Dense(dl,activation=\"relu\"))\n",
        "\tmodel.add(Dense(6))\n",
        "\tadam = Adam(lr=learning_rate)\n",
        "\tmodel.compile(loss='mse',optimizer=adam)\n",
        "\treturn model\n",
        "\n",
        "class Pong:\n",
        "\tdef __init__(self):\n",
        "\t\tself.env = gym.make('Pong-v0')\n",
        "\t\tself.epsilon = 1\n",
        "\t\tself.buffer = Buffer(50000)\n",
        "\t\tself.dqn = DQN(self.buffer)\n",
        "\t\tself.copy_period = 40000\n",
        "\t\tself.itr = 0\n",
        "\t\tself.eps_step = 0.0000009\n",
        "\n",
        "\tdef sample_action(self,s):\n",
        "\t\tif random.random() < self.epsilon:\n",
        "\t\t\treturn self.env.action_space.sample()\n",
        "\t\treturn np.argmax(self.dqn.predict(s)[0])\n",
        "\t\n",
        "\t\n",
        "\tdef play_one_episode(self):\n",
        "\t\tobservation = self.env.reset()\n",
        "\t\tdone = False\n",
        "\t\tstate = []\n",
        "\t\tupdate_state(state,observation)\n",
        "\t\tprv_state = []\n",
        "\t\ttotal_reward = 0\n",
        "\t\twhile not done:\n",
        "\t\t\t\n",
        "\t\t\tif len(state) < 4:\n",
        "\t\t\t\taction = self.env.action_space.sample()\n",
        "\t\t\telse:\n",
        "\t\t\t\taction = self.sample_action(state)\n",
        "        \n",
        "\t\t\tprv_state.append(state[-1])\n",
        "\t\t\tif len(prv_state) > 4:\n",
        "\t\t\t\tprv_state.pop(0)\n",
        "\t\t\tobservation, reward, done, _ = self.env.step(action)\n",
        "\n",
        "\t\t\tupdate_state(state,observation)\n",
        "\t\t\tif len(state) == 4 and len(prv_state) == 4:\n",
        "\t\t\t\tself.buffer.add(prv_state,action,reward,state,done)\n",
        "\t\t\ttotal_reward += reward\n",
        "\t\t\t\n",
        "\t\t\tself.itr += 1\n",
        "\t\t\tif self.itr % 4 == 0:\n",
        "\t\t\t\tself.dqn.train()\n",
        "\t\t\tself.epsilon = max(0.1,self.epsilon-self.eps_step)\n",
        "\t\t\tif self.itr % self.copy_period == 0:\n",
        "\t\t\t\tself.dqn.copy_network()\n",
        "\t\treturn total_reward\n",
        "\n",
        "\n",
        "\t\n",
        "def downsample(observation):\n",
        "\ts = cv2.cvtColor(observation[30:,:,:], cv2.COLOR_BGR2GRAY)\n",
        "\ts = cv2.resize(s, (80,80), interpolation = cv2.INTER_AREA) \n",
        "\ts = s/255.0\n",
        "\treturn s\n",
        "\t\t\n",
        "def update_state(state,observation):\n",
        "\tds_observation = downsample(observation)\n",
        "\tstate.append(ds_observation)\n",
        "\tif len(state) > 4:\n",
        "\t\tstate.pop(0)\n",
        "\n",
        "\n",
        "p = Pong()\n",
        "for i in range(100000):\n",
        "\ttotal_reward = p.play_one_episode()\n",
        "\tprint(\"episode total reward:\",total_reward)\n",
        "\tif i%100 == 0:\n",
        "\t\tprint(\"Saving the model\")\n",
        "\t\tp.dqn.model.save(\"model-{}.h5\".format(i))\n",
        "\t"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}